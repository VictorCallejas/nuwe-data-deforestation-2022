{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "### Goal\n",
    "- Play with model architecture and hyperparameterization (train/val split)\n",
    "- Train the best model on all training data\n",
    "- Generate predictions for test dataset\n",
    "\n",
    "### Discussion\n",
    "- due to time/gpu limitations the f1-score might be limited, a ton of improvements can be tested as Ensemble models, Data Augmetations, Transformer backbone pre-trainig, SWA...\n",
    "\n",
    "### Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.data import NuweDataset\n",
    "from src.model import focalnet_tiny_srf\n",
    "from src.train import train_epoch, valid_epoch, generate_test_results\n",
    "from src.loss import F1_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1714, 5) test (635, 4)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test = pd.read_csv(DATA_PATH +'test.csv')\n",
    "\n",
    "print(f'train {train.shape} test {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') \n",
    "num_workers = 0\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = NuweDataset(data = train, directory = DATA_PATH)\n",
    "full_train_dataloader = DataLoader(full_train_dataset, \n",
    "                          batch_size  = batch_size, \n",
    "                          shuffle = False, \n",
    "                          num_workers = num_workers,\n",
    "                          pin_memory = True)\n",
    "\n",
    "test_dataset = NuweDataset(data = test, directory = DATA_PATH, train = False)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                          batch_size = batch_size, \n",
    "                          shuffle = False, \n",
    "                          num_workers = num_workers,\n",
    "                          pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1148, 5) (566, 5)\n"
     ]
    }
   ],
   "source": [
    "train, val = train_test_split(train, test_size=0.33, random_state=42, stratify=train.label)\n",
    "print(f'{train.shape} {val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NuweDataset(data = train, directory = DATA_PATH)\n",
    "val_dataset = NuweDataset(data = val, directory = DATA_PATH)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                          batch_size  = batch_size, \n",
    "                          shuffle = False, \n",
    "                          num_workers = num_workers,\n",
    "                          pin_memory = True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                          batch_size = batch_size, \n",
    "                          shuffle = False, \n",
    "                          num_workers = num_workers,\n",
    "                          pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NuweModel(\n",
       "  (image_backbone): FocalNet(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        dim=96, input_resolution=(56, 56), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): FocalNetBlock(\n",
       "            dim=96, input_resolution=(56, 56), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=96\n",
       "              (f): Linear(in_features=96, out_features=195, bias=True)\n",
       "              (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): FocalNetBlock(\n",
       "            dim=96, input_resolution=(56, 56), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=96\n",
       "              (f): Linear(in_features=96, out_features=195, bias=True)\n",
       "              (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchEmbed(\n",
       "          (proj): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=192, input_resolution=(28, 28), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): FocalNetBlock(\n",
       "            dim=192, input_resolution=(28, 28), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=192\n",
       "              (f): Linear(in_features=192, out_features=387, bias=True)\n",
       "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): FocalNetBlock(\n",
       "            dim=192, input_resolution=(28, 28), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=192\n",
       "              (f): Linear(in_features=192, out_features=387, bias=True)\n",
       "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchEmbed(\n",
       "          (proj): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=384, input_resolution=(14, 14), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): FocalNetBlock(\n",
       "            dim=384, input_resolution=(14, 14), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=384\n",
       "              (f): Linear(in_features=384, out_features=771, bias=True)\n",
       "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): FocalNetBlock(\n",
       "            dim=384, input_resolution=(14, 14), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=384\n",
       "              (f): Linear(in_features=384, out_features=771, bias=True)\n",
       "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): FocalNetBlock(\n",
       "            dim=384, input_resolution=(14, 14), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=384\n",
       "              (f): Linear(in_features=384, out_features=771, bias=True)\n",
       "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): FocalNetBlock(\n",
       "            dim=384, input_resolution=(14, 14), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=384\n",
       "              (f): Linear(in_features=384, out_features=771, bias=True)\n",
       "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): FocalNetBlock(\n",
       "            dim=384, input_resolution=(14, 14), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=384\n",
       "              (f): Linear(in_features=384, out_features=771, bias=True)\n",
       "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): FocalNetBlock(\n",
       "            dim=384, input_resolution=(14, 14), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=384\n",
       "              (f): Linear(in_features=384, out_features=771, bias=True)\n",
       "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchEmbed(\n",
       "          (proj): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        dim=768, input_resolution=(7, 7), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): FocalNetBlock(\n",
       "            dim=768, input_resolution=(7, 7), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=768\n",
       "              (f): Linear(in_features=768, out_features=1539, bias=True)\n",
       "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): FocalNetBlock(\n",
       "            dim=768, input_resolution=(7, 7), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (modulation): FocalModulation(\n",
       "              dim=768\n",
       "              (f): Linear(in_features=768, out_features=1539, bias=True)\n",
       "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (focal_layers): ModuleList(\n",
       "                (0): Sequential(\n",
       "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Sequential(\n",
       "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (image_proj): Linear(in_features=768, out_features=30, bias=True)\n",
       "  (year): Embedding(16, 5)\n",
       "  (neighbors_proj): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=40, out_features=10, bias=True)\n",
       "    (4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=10, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NuweModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(NuweModel, self).__init__()\n",
    "        \n",
    "        self.image_backbone = focalnet_tiny_srf(pretrained=True)\n",
    "        self.image_backbone.head = nn.Identity()\n",
    "        self.image_proj = nn.Linear(768, 30)\n",
    "        \n",
    "        self.year = nn.Embedding(16, 5)\n",
    "        \n",
    "        self.neighbors_proj = nn.Linear(3, 5)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(40,40),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40,10),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image, year, neighbors_ctx):\n",
    "        \n",
    "        x_img = self.image_backbone(image)\n",
    "        x_img = self.image_proj(x_img)\n",
    "        \n",
    "        x_year = self.year(year)\n",
    "        x_neighbors_ctx = self.neighbors_proj(neighbors_ctx)\n",
    "        \n",
    "        x = torch.cat([x_img, x_year, x_neighbors_ctx], dim=1)\n",
    "        \n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = NuweModel()\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = F1_Loss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr = 3e-6\n",
    "        )\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15000, gamma=0.5) # Not using\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32944e741dad48949e6a7dde833ea542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mepochs,leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m epoch_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, epochs):\n\u001b[1;32m----> 4\u001b[0m         train_f1, train_acc, train_f1_na, avg_train_loss \u001b[39m=\u001b[39m train_epoch(model, train_dataloader, device, criterion, optimizer, scheduler\u001b[39m=\u001b[39;49mscheduler)\n\u001b[0;32m      5\u001b[0m         valid_f1, valid_acc, valid_f1_na, avg_valid_loss \u001b[39m=\u001b[39m valid_epoch(model, val_dataloader, device, criterion)\n\u001b[0;32m      7\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTRAIN Epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch_i\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtrain_f1\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtrain_f1_na\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mavg_train_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\src\\train.py:36\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_dataloader, device, criterion, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     33\u001b[0m total_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m2.0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     38\u001b[0m logits\u001b[39m.\u001b[39mextend(b_logits\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     39\u001b[0m ground_truth\u001b[39m.\u001b[39mextend(b_labels\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    158\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    160\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 162\u001b[0m     adamw(params_with_grad,\n\u001b[0;32m    163\u001b[0m           grads,\n\u001b[0;32m    164\u001b[0m           exp_avgs,\n\u001b[0;32m    165\u001b[0m           exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m           max_exp_avg_sqs,\n\u001b[0;32m    167\u001b[0m           state_steps,\n\u001b[0;32m    168\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    169\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    170\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    171\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    172\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    173\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 219\u001b[0m func(params,\n\u001b[0;32m    220\u001b[0m      grads,\n\u001b[0;32m    221\u001b[0m      exp_avgs,\n\u001b[0;32m    222\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    223\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    224\u001b[0m      state_steps,\n\u001b[0;32m    225\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    226\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    227\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    228\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    229\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    230\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    232\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\adamw.py:316\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    314\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    318\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total=epochs,leave=False) as pbar:\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        train_f1, train_acc, train_f1_na, avg_train_loss = train_epoch(model, train_dataloader, device, criterion, optimizer, scheduler=scheduler)\n",
    "        valid_f1, valid_acc, valid_f1_na, avg_valid_loss = valid_epoch(model, val_dataloader, device, criterion)\n",
    "        \n",
    "        print(f'TRAIN Epoch: {epoch_i} {train_f1:.3f} {train_acc:.3f} {train_f1_na} {avg_train_loss:.3f}')\n",
    "        print(f'VAL Epoch: {epoch_i} {valid_f1:.3f} {valid_acc:.3f} {valid_f1_na} {avg_valid_loss:.3f}')  \n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'artifacts/model2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final train on all train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NuweModel()\n",
    "model = model.to(device)\n",
    "\n",
    "#criterion = F1_Loss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr = 3e-6\n",
    "        )\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15000, gamma=0.5) # Not being used\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dcb31d0ecc45ba853cd722084aa57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch: 0 0.316 0.440 [0.61182033 0.19897084 0.1369863 ] 1.063\n",
      "TRAIN Epoch: 1 0.452 0.545 [0.69189714 0.38528897 0.27889447] 0.993\n",
      "TRAIN Epoch: 2 0.517 0.593 [0.7235732  0.44680851 0.37926973] 0.951\n",
      "TRAIN Epoch: 3 0.572 0.638 [0.75642965 0.50619469 0.45454545] 0.918\n",
      "TRAIN Epoch: 4 0.613 0.669 [0.77484787 0.55555556 0.50873362] 0.896\n",
      "TRAIN Epoch: 5 0.637 0.686 [0.77981651 0.58536585 0.54448017] 0.878\n",
      "TRAIN Epoch: 6 0.652 0.701 [0.79532758 0.6015625  0.55966209] 0.860\n",
      "TRAIN Epoch: 7 0.677 0.718 [0.79573171 0.6352459  0.60082305] 0.846\n",
      "TRAIN Epoch: 8 0.695 0.734 [0.81256332 0.66252588 0.60968074] 0.832\n",
      "TRAIN Epoch: 9 0.721 0.751 [0.81706064 0.69344609 0.65213082] 0.824\n",
      "TRAIN Epoch: 10 0.734 0.765 [0.82829318 0.70212766 0.67130089] 0.807\n",
      "TRAIN Epoch: 11 0.740 0.771 [0.83367769 0.70464135 0.68172888] 0.802\n",
      "TRAIN Epoch: 12 0.754 0.782 [0.84188699 0.72765957 0.69387755] 0.790\n",
      "TRAIN Epoch: 13 0.768 0.792 [0.8496124  0.75       0.70395371] 0.783\n",
      "TRAIN Epoch: 14 0.776 0.797 [0.84731631 0.75615213 0.72316384] 0.772\n",
      "TRAIN Epoch: 15 0.789 0.809 [0.85863328 0.77433628 0.73276676] 0.764\n",
      "TRAIN Epoch: 16 0.787 0.806 [0.85297185 0.77477477 0.73358349] 0.754\n",
      "TRAIN Epoch: 17 0.809 0.821 [0.86237572 0.80995475 0.75348837] 0.747\n",
      "TRAIN Epoch: 18 0.807 0.825 [0.86924299 0.78935698 0.76286765] 0.741\n",
      "TRAIN Epoch: 19 0.829 0.843 [0.87987186 0.81431767 0.79241877] 0.730\n",
      "TRAIN Epoch: 20 0.825 0.836 [0.87067589 0.82326622 0.78221416] 0.728\n",
      "TRAIN Epoch: 21 0.828 0.841 [0.87706227 0.81797753 0.78804348] 0.723\n",
      "TRAIN Epoch: 22 0.843 0.851 [0.88533333 0.84668192 0.79569892] 0.716\n",
      "TRAIN Epoch: 23 0.849 0.856 [0.88663102 0.85450346 0.80711111] 0.709\n",
      "TRAIN Epoch: 24 0.846 0.858 [0.88900804 0.8317757  0.81762115] 0.707\n",
      "TRAIN Epoch: 25 0.860 0.867 [0.89402905 0.8618267  0.82486865] 0.700\n",
      "TRAIN Epoch: 26 0.867 0.873 [0.89696312 0.87037037 0.83506944] 0.693\n",
      "TRAIN Epoch: 27 0.875 0.886 [0.91187739 0.85648148 0.85714286] 0.691\n",
      "TRAIN Epoch: 28 0.875 0.881 [0.90623306 0.87703016 0.84201389] 0.685\n",
      "TRAIN Epoch: 29 0.882 0.886 [0.90909091 0.88888889 0.84874676] 0.679\n",
      "TRAIN Epoch: 30 0.883 0.889 [0.91216585 0.88167053 0.85395189] 0.678\n",
      "TRAIN Epoch: 31 0.889 0.897 [0.91912568 0.87885986 0.86830926] 0.673\n",
      "TRAIN Epoch: 32 0.895 0.898 [0.91639164 0.8957346  0.87205387] 0.671\n",
      "TRAIN Epoch: 33 0.896 0.901 [0.92028587 0.89099526 0.87615838] 0.668\n",
      "TRAIN Epoch: 34 0.900 0.905 [0.92273731 0.89311164 0.88368201] 0.663\n",
      "TRAIN Epoch: 35 0.892 0.899 [0.92019813 0.88361045 0.87226891] 0.660\n",
      "TRAIN Epoch: 36 0.898 0.904 [0.92511013 0.89411765 0.87615838] 0.655\n",
      "TRAIN Epoch: 37 0.904 0.910 [0.92682927 0.8952381  0.88870432] 0.653\n",
      "TRAIN Epoch: 38 0.909 0.914 [0.93097736 0.9060241  0.89018303] 0.648\n",
      "TRAIN Epoch: 39 0.907 0.912 [0.92623405 0.89855072 0.89512799] 0.648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mepochs,leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m epoch_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, epochs):\n\u001b[1;32m----> 4\u001b[0m         train_f1, train_acc, train_f1_na, avg_train_loss \u001b[39m=\u001b[39m train_epoch(model, full_train_dataloader, device, criterion, optimizer, scheduler\u001b[39m=\u001b[39;49mscheduler)\n\u001b[0;32m      5\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTRAIN Epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch_i\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtrain_f1\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtrain_f1_na\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mavg_train_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m         pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\src\\train.py:36\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_dataloader, device, criterion, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     33\u001b[0m total_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m2.0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     38\u001b[0m logits\u001b[39m.\u001b[39mextend(b_logits\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     39\u001b[0m ground_truth\u001b[39m.\u001b[39mextend(b_labels\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    158\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    160\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 162\u001b[0m     adamw(params_with_grad,\n\u001b[0;32m    163\u001b[0m           grads,\n\u001b[0;32m    164\u001b[0m           exp_avgs,\n\u001b[0;32m    165\u001b[0m           exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m           max_exp_avg_sqs,\n\u001b[0;32m    167\u001b[0m           state_steps,\n\u001b[0;32m    168\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    169\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    170\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    171\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    172\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    173\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 219\u001b[0m func(params,\n\u001b[0;32m    220\u001b[0m      grads,\n\u001b[0;32m    221\u001b[0m      exp_avgs,\n\u001b[0;32m    222\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    223\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    224\u001b[0m      state_steps,\n\u001b[0;32m    225\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    226\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    227\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    228\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    229\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    230\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    232\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\repos\\nuwe-data-deforestation-2022\\env\\lib\\site-packages\\torch\\optim\\adamw.py:316\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    314\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    318\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total=epochs,leave=False) as pbar:\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        train_f1, train_acc, train_f1_na, avg_train_loss = train_epoch(model, full_train_dataloader, device, criterion, optimizer, scheduler=scheduler)\n",
    "        print(f'TRAIN Epoch: {epoch_i} {train_f1:.3f} {train_acc:.3f} {train_f1_na} {avg_train_loss:.3f}')\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'artifacts/model3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Test Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69      2\n",
       "469     1\n",
       "6       0\n",
       "351     2\n",
       "1001    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = generate_test_results(model, test_dataloader, device)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target\n",
       "69         2\n",
       "469        1\n",
       "6          0\n",
       "351        2\n",
       "1001       2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set submission format\n",
    "df = pd.DataFrame(predictions)\n",
    "df.columns = ['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "df.to_json('predictions.json', orient='columns', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d76c160639fedf4b6ab9278b4676e11dfe78feaaf3e9d6e357664936dc82500"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
